# AI-Powered Code Analysis Multi-Agent System
# Copy this file to .env and fill in your values

# ===========================================
# LLM Configuration (choose one)
# ===========================================

# Option 1: OpenAI API (cloud)
OPENAI_API_KEY=your-openai-api-key-here

# Option 2: Ollama (local) - no API key needed
# Just ensure Ollama is running: ollama serve
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# ===========================================
# LangSmith Configuration (optional but recommended)
# ===========================================
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-api-key-here
LANGCHAIN_PROJECT=code-analyzer-capstone

# ===========================================
# Application Settings
# ===========================================

# Backend server
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000

# Frontend URL (for CORS)
FRONTEND_URL=http://localhost:5173

# Issue storage directory
ISSUES_DIR=./issues

# ===========================================
# Analysis Settings
# ===========================================

# Maximum file size to analyze (in bytes)
MAX_FILE_SIZE=1048576

# File patterns to ignore during analysis
IGNORE_PATTERNS=__pycache__,*.pyc,.git,node_modules,venv,.venv,dist,build

# Default file types to analyze
DEFAULT_FILE_TYPES=*.py
